{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection Code 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from math import ceil\n",
    "\n",
    "# Creates a new .csv file that the data will be written to\n",
    "csv_file = open('PMscraping.csv', 'w', encoding=\"UTF-8\", newline=\"\")\n",
    "writer = csv.writer(csv_file)\n",
    "\n",
    "# Define the variables (future data frame columns) to be scraped\n",
    "writer.writerow(['Job Title', 'location', 'Avg Salary for the job title/location', 'Summary'])\n",
    "\n",
    "#Invoking the webdriver\n",
    "locationOfWebdriver = \"C:/Users/nisha/Desktop/chromedriver.exe\"\n",
    "driver = webdriver.Chrome(locationOfWebdriver)\n",
    "\n",
    "# The sleep time has been set to 5 secs since loading entirely new page has proven to take longer\n",
    "# than iterating through the various job postings within a single page.\n",
    "page=\"https://www.indeed.com/jobs?q=product+manager&l=San+Francisco,+CA\"\n",
    "driver.get(page)\n",
    "time.sleep(5)\n",
    "try:\n",
    "    # Find total number of product manager jobs in San Francisco Area on the portal\n",
    "    # Each page defaults to showing 10 reviews, so take the ceiling of the total number of job postings divided by 10\n",
    "    # to get the number of pages\n",
    "    total = driver.find_element_by_xpath('//div[@id = \"searchCountPages\"]').text\n",
    "    jobs_count = ''.join([i for i in total.split()[3] if i.isdigit()])\n",
    "    n = int(ceil(float(jobs_count) / 10))\n",
    "    \n",
    "    # Iterate through all the pages of job posting for the product manager role in San Francisco Area\n",
    "    index = 0\n",
    "    k = 1  \n",
    "    while index <= 10*(n-108):    # n will come out to be 157 but fetching details only for the first 50 pages so 500 records\n",
    "        driver.get(page + \"&start=\" + str(index))\n",
    "        time.sleep(2)\n",
    "        print(\"Scraping \" + \"page : \" + str(k))\n",
    "        k = k + 1\n",
    "        try:\n",
    "            index = index + 10\n",
    "            # Find all the job postings:\n",
    "            Jobs = driver.find_elements_by_xpath('//div[@class=\"jobsearch-SerpJobCard unifiedRow row result clickcard\"]')\n",
    "            for Job in Jobs:\n",
    "                # Initialize an empty dictionary for each job posting\n",
    "                Jobs_dict = {}\n",
    "\n",
    "                # Find xpaths of the fields desired as columns in future data frame\n",
    "                # We use the try/except statements to account for the fact that the job postings are not required to have\n",
    "                # all the fields listed below, and if a job posting does not have a certain field we wish to make the\n",
    "                # corresponding field blank in that particular row, rather than quit upon receiving an error.\n",
    "                try:\n",
    "                    Job_Title = Job.find_element_by_xpath('.//div[@class =\"title\"]').text\n",
    "                except:\n",
    "                    Job_Title = \"\"\n",
    "                try:\n",
    "                    Location = Job.find_element_by_xpath('.//div[@class =\"sjcl\"]//span[@class=\"location accessible-contrast-color-location\"]').text\n",
    "                except:\n",
    "                    Location = \"\"\n",
    "                try:\n",
    "                    Sal_Link = Job.find_element_by_xpath('.//div[@class =\"tab-container\"]//li[2]//a').get_attribute('href')\n",
    "                    driver1 = webdriver.Chrome(locationOfWebdriver)\n",
    "                    driver1.get(Sal_Link) #Fetching salaries from the new link obtained where the average salary for the \n",
    "                    time.sleep(2)         #same post and location has been provided\n",
    "                    Avg_Sal = driver1.find_element_by_xpath('.//div[@class =\"cmp-sal-salary\"]').text\n",
    "                    driver1.close()\n",
    "                except:\n",
    "                    Avg_Sal = \"\"\n",
    "                try:\n",
    "                    Summary = Job.find_element_by_xpath('.//div[@class =\"summary\"]').text\n",
    "                except:\n",
    "                    Summary = \"\"\n",
    "\n",
    "                # Write the results of the above to a dictionary. Note that each overall job posting will have its\n",
    "                # own dictionary, but all dictionaries for all the rows will all have the same keys. This\n",
    "                # allows Selenium to write the contents of these dictionaries into a coherent .csv file\n",
    "                Jobs_dict['Job Title'] = Job_Title\n",
    "                Jobs_dict['Location'] = Location\n",
    "                Jobs_dict['Avg_Sal'] = Avg_Sal\n",
    "                Jobs_dict['Summary'] = Summary\n",
    "                writer.writerow(Jobs_dict.values())\n",
    "\n",
    "            # If an error is thrown unrelated to the above variables, print the error to the terminal\n",
    "            # console, close the .csv file, and break the while loop.\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            csv_file.close()\n",
    "            driver.close()\n",
    "            break\n",
    "\n",
    "# If an error is thrown , print the error to the terminal\n",
    "# console, close the .csv file.\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    csv_file.close()\n",
    "    driver.close()\n",
    "    \n",
    "#Closing the csv file and the driver that has been opened\n",
    "csv_file.close()\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection Code 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys \n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from math import ceil\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--ignore-certificate-errors')\n",
    "options.add_argument(\"--test-type\")\n",
    "options.binary_location = \"/usr/bin/chromium\"\n",
    "\n",
    "# Creates a new .csv file that the data will be written to\n",
    "csv_file = open('hyderabad.csv', 'w', encoding=\"UTF-8\", newline=\"\")\n",
    "writer = csv.writer(csv_file)\n",
    "\n",
    "# Define the variables (future data frame columns) to be scraped\n",
    "writer.writerow(['Name of the company', 'Number of Positions','Segmentation', 'Skills companies are looking for'])\n",
    "\n",
    "#Invoking the webdriver\n",
    "locationOfWebdriver = \"C:/Users/nisha/Desktop/chromedriver.exe\"\n",
    "driver = webdriver.Chrome(locationOfWebdriver)\n",
    "\n",
    "#Opening the naukri.com url with the help of timer and making use of sleep function\n",
    "# to halt execution of next set of commands for 15 sec so that page gets loaded fully meanwhile\n",
    "driver.get(\"https://www.naukri.com\")\n",
    "time.sleep(15)\n",
    "\n",
    "#Generating handle for the later button that appears on the popup\n",
    "#Clicking on the later button through the handle generated\n",
    "Later_click = driver.find_element_by_xpath('//span[@id = \"block\"]')\n",
    "Later_click.click()\n",
    "\n",
    "#Generating handle for the search button and clicking on the same\n",
    "#through the handle generated in order to enter the segment\n",
    "search_job = driver.find_element_by_xpath('//span[@class = \"searchJob\"]')\n",
    "search_job.click()\n",
    "time.sleep(3)\n",
    "\n",
    "#Searching for the position handle and the location handle\n",
    "#Supplying values Information Technology and Hyderabad in Jobs and location respectively\n",
    "#Clicking on the enter key in order to generate the results\n",
    "search_position = driver.find_element_by_name('qp')\n",
    "search_location = driver.find_element_by_name('ql')\n",
    "search_position.send_keys(\"Information Technology\")\n",
    "time.sleep(2)\n",
    "search_location.send_keys(\"Hyderabad\") \n",
    "search_position.send_keys(Keys.RETURN)\n",
    "time.sleep(5)\n",
    "\n",
    "#Fetching the down arrow handle and selecting the sort by date option \n",
    "#in order to get the new job posting appear on top \n",
    "driver.find_element_by_xpath('//em[@class = \"downArrow\"]').click()\n",
    "driver.find_element_by_xpath('/html/body/div[5]/div/div[3]/div[1]/div[1]/div/div[2]/div/div/ul/li[1]').click()\n",
    "try:\n",
    "    # Find total number of Information Technology jobs in Hyderabad Area on the portal(www.naukri.com)\n",
    "    # Each page defaults to showing 50 reviews, so take the ceiling of the total number of job postings divided by 50\n",
    "    # to get the number of pages\n",
    "    total = driver.find_element_by_xpath('//div[@class = \"srp_count\"]//span[@class = \"cnt\"]').text\n",
    "    jobs_count = float(str(total).split(\" \")[2])\n",
    "    n = int(ceil(float(jobs_count) / 50))\n",
    "    print(n)\n",
    "    k = 1  \n",
    "    while k <= n: # Initializing k with 1 and looping it till it exhausts all the pages \n",
    "        print(\"Scraping \" + \"page : \" + str(k))  # Printing the page no which is being scraped\n",
    "        try:\n",
    "            p = 1\n",
    "            # Fetching the handle for all the job postings available on the page opened by the driver\n",
    "            job_postings = driver.find_elements_by_xpath('//div[@type = \"tuple\"]')\n",
    "            for job in job_postings: # Taking one job at a time to extract the info related to posting\n",
    "                print(\"Scraping \" + \"job posting #\" + str(p) + \" of page : \" + str(k))\n",
    "                p = p + 1\n",
    "                Jobs_dict = {}       # Creating an empty dictionary by the name \"Jobs_dict\"\n",
    "                try:\n",
    "                    #Fetching Company name \n",
    "                    Company = job.find_element_by_xpath('.//span[@class = \"org\"]').text\n",
    "                except:\n",
    "                    Company = \"\"\n",
    "                try:\n",
    "                    #First fetching the link where No_of_Openings and Key_skills features are embedded\n",
    "                    #Opening the fetched link with the help of driver and then storing the No_of_openings\n",
    "                    #to get the positions available for that role inside that company\n",
    "                    Opening_link = job.find_element_by_xpath('.//span[@class =\"content\"]//li//a').get_attribute('href')\n",
    "                    driver1 = webdriver.Chrome(locationOfWebdriver)\n",
    "                    driver1.get(Opening_link)\n",
    "                    time.sleep(5)\n",
    "                    No_of_openings = driver1.find_element_by_xpath('//div[@class = \"jd-stats\"]//span[2]//span').text\n",
    "                except:\n",
    "                    No_of_openings = \"\"\n",
    "                try:\n",
    "                    #Segmentation is the field in which we are looking for jobs (in this case it is Information technology)\n",
    "                    Segmentation = job.find_element_by_xpath('//div[@class = \"small_title\"]//span[@class = \"frst\"]').text\n",
    "                except:\n",
    "                    Segmentation= \"\"\n",
    "                try:\n",
    "                    #Fetching the key skills listed by the company for that particular position\n",
    "                    key_skills = driver1.find_elements_by_xpath('//div//a[@class = \"chip clickable\"]')\n",
    "                    skills_required = ','.join([key.text for key in key_skills])\n",
    "                    driver1.close()\n",
    "                except:\n",
    "                    skills_required = \"\" \n",
    "                \n",
    "                # Write the results of the above to a dictionary. Note that each overall job posting will have its\n",
    "                # own dictionary, but all dictionaries for all the rows will all have the same keys. This\n",
    "                # allows Selenium to write the contents of these dictionaries into a coherent .csv file    \n",
    "                Jobs_dict['Job Title'] = Company\n",
    "                Jobs_dict['No_of_openings'] = No_of_openings\n",
    "                Jobs_dict['Segmentation'] = Segmentation\n",
    "                Jobs_dict['skills_required'] = skills_required\n",
    "                writer.writerow(Jobs_dict.values())\n",
    "            \n",
    "            #Fetching the Next button handle.It will be different for the first page as compared to the rest of the \n",
    "            #pages as first page will not be having previous button whereas all the other except the last page will \n",
    "            #be having the previous and next button.Last page will not be having next button for the obvious reasons.\n",
    "            if(k==1):\n",
    "                Next_button = driver.find_element_by_xpath('//div[@class = \"pagination\"]//button[@class = \"grayBtn\"]')\n",
    "                Next_button.click()\n",
    "                time.sleep(5)    \n",
    "            elif(k==n):\n",
    "                break\n",
    "            else:\n",
    "                Next_previous_button = driver.find_element_by_xpath('//div[@class = \"pagination\"]//a[2]//button[@class = \"grayBtn\"]')\n",
    "                Next_previous_button.click()\n",
    "                time.sleep(5)\n",
    "            k = k+1\n",
    "            \n",
    "        # If an error is thrown unrelated to the above variables, print the error to the terminal\n",
    "        # console, close the .csv file, and break the while loop.\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            csv_file.close()\n",
    "            driver.close()\n",
    "            break\n",
    "\n",
    "# If an error is thrown unrelated to the above variables, print the error to the terminal\n",
    "# console, close the .csv file, and break the while loop.           \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    csv_file.close()\n",
    "    driver.close()\n",
    "\n",
    "#Closing the csv file and the driver that has been opened\n",
    "csv_file.close()\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection Code 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the required libraries\n",
    "from newsapi import NewsApiClient\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "#Supplying the api_key genrated to the NewsApiClient function and storing it in the newsapi variable\n",
    "newsapi = NewsApiClient(api_key='73490089db3547b7bf4742da780c48b2')\n",
    "\n",
    "#Printing newsapi variable\n",
    "newsapi \n",
    "\n",
    "#Buiding the API query (topic : economy , Source : Wall Street Journal articles , data extracted for the last 6 months , \n",
    "# sorted according to the published date (newer articles will come first , older articles will appear later))\n",
    "all_articles = newsapi.get_everything(q='Economy',\n",
    "                                      sources='the-wall-street-journal',\n",
    "                                      domains='wsj.com',\n",
    "                                      from_param='2019-09-15',\n",
    "                                      to='2019-10-14',\n",
    "                                      language='en',\n",
    "                                      sort_by='publishedAt',\n",
    "                                      page=5)\n",
    "\n",
    "#Printing all_articles variables i.e. dispalying it's contents\n",
    "all_articles\n",
    "\n",
    "# Viewing only the details related to first article\n",
    "all_articles['articles'][0]\n",
    "\n",
    "# Creates a new .csv file that the data will be written to\n",
    "csv_file = open('news_api.csv', 'w', encoding=\"UTF-8\", newline=\"\")\n",
    "writer = csv.writer(csv_file)\n",
    "\n",
    "# Define the variables (future data frame columns) to be scraped\n",
    "writer.writerow(['source_id', 'source_name', 'author_name', 'title' , 'description'])\n",
    "\n",
    "# Iterating through all the articles (max allowed : 20 ) satisfying the filtration criteria.\n",
    "# Initialize an empty dictionary for each article.\n",
    "# Fetching details related to an article and writing the results of the above to a dictionary\n",
    "# Note that each article will have its own dictionary, but all dictionaries for all the rows will all have the same keys. \n",
    "# This allows Selenium to write the contents of these dictionaries into a coherent .csv file\n",
    "articles_dict = {}\n",
    "for i in range(0,20,1):\n",
    "    articles_dict['source_id'] = all_articles['articles'][i]['source']['id']\n",
    "    articles_dict['source_name'] = all_articles['articles'][i]['source']['name']\n",
    "    articles_dict['author_name'] = all_articles['articles'][i]['author']\n",
    "    articles_dict['title'] = all_articles['articles'][i]['title']\n",
    "    articles_dict['description'] = all_articles['articles'][i]['description']\n",
    "    writer.writerow(articles_dict.values())\n",
    "\n",
    "#Close the csv file \n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
